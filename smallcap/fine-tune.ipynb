{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjustinwu314\u001b[0m (\u001b[33mjustin-wu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/smallcap/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#make sure to cd into smallcap-main\n",
    "#load in datasets\n",
    "import importlib\n",
    "#importlib.reload(src.utils)\n",
    "import torch\n",
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "#sys.path.append('/home/ec2-user/smallcap-main/src')\n",
    "from transformers.models.auto.configuration_auto import AutoConfig\n",
    "from transformers import AutoTokenizer, CLIPFeatureExtractor, AutoModel, AutoModelForCausalLM\n",
    "from transformers import Seq2SeqTrainer, default_data_collator, Seq2SeqTrainingArguments\n",
    "\n",
    "from transformers import VisionEncoderDecoderModel, CLIPModel, CLIPVisionModel,EncoderDecoderModel\n",
    "from src.vision_encoder_decoder import SmallCap, SmallCapConfig\n",
    "from src.gpt2 import ThisGPT2Config, ThisGPT2LMHeadModel\n",
    "from src.xglm import ThisXGLMConfig, ThisXGLMForCausalLM\n",
    "from src.opt import ThisOPTConfig, ThisOPTForCausalLM\n",
    "\n",
    "from src.utils import *\n",
    "from transformers import ViTFeatureExtractor, AutoTokenizer, CLIPFeatureExtractor, AutoModel, AutoModelForCausalLM\n",
    "from transformers.models.auto.configuration_auto import AutoConfig\n",
    "from src.vision_encoder_decoder import SmallCap, SmallCapConfig\n",
    "from src.gpt2 import ThisGPT2Config, ThisGPT2LMHeadModel\n",
    "from src.utils import *\n",
    "import pandas\n",
    "import src.utils\n",
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "from pandas import *\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(tokenizer, max_length, features_dir, annotations_path, experiments_dir, encoder_name, decoder_name, attention_size, train_decoder,\n",
    "     disable_rag, k, retrieval_encoder, captions_path, template_path, n_epochs, lr, batch_size, gradient_steps):\n",
    "\n",
    "    data = load_data_for_training(annotations_path, captions_path)\n",
    "    train_df = pd.DataFrame(data['train'])\n",
    "\n",
    "    train_dataset = TrainDataset(\n",
    "                           df=train_df,\n",
    "                           features_path=os.path.join(features_dir,'train.hdf5'),\n",
    "                           tokenizer=tokenizer,\n",
    "                           rag=not disable_rag,\n",
    "                           template_path=template_path,\n",
    "                           k=k,\n",
    "                           max_target_length=max_length)\n",
    "\n",
    "    return train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_auxiliaries():\n",
    "\n",
    "    \n",
    "    # load feature extractor\n",
    "    feature_extractor = CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    # load and configure tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "    tokenizer.pad_token = '!'\n",
    "    tokenizer.eos_token = '.'\n",
    "\n",
    "    # load model\n",
    "    AutoConfig.register(\"this_opt\", ThisOPTConfig)\n",
    "    AutoModel.register(ThisOPTConfig, ThisOPTForCausalLM)\n",
    "    AutoModelForCausalLM.register(ThisOPTConfig, ThisOPTForCausalLM)\n",
    "    AutoConfig.register(\"smallcap\", SmallCapConfig)\n",
    "    AutoModel.register(SmallCapConfig, SmallCap)\n",
    "    model = AutoModel.from_pretrained(\"Yova/SmallCap7M\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # count trainable parameters\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    num_trainable_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    print('Training a model with {} trainable parameters.'.format(num_trainable_params))\n",
    "\n",
    "    return model, tokenizer, feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_auxiliaries():\n",
    "\n",
    "    \n",
    "    # load feature extractor\n",
    "    feature_extractor = CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    # load and configure tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = '!'\n",
    "    tokenizer.eos_token = '.'\n",
    "\n",
    "    # load model\n",
    "    AutoConfig.register(\"this_gpt2\", ThisGPT2Config)\n",
    "    AutoModel.register(ThisGPT2Config, ThisGPT2LMHeadModel)\n",
    "    AutoModelForCausalLM.register(ThisGPT2Config, ThisGPT2LMHeadModel)\n",
    "    AutoConfig.register(\"smallcap\", SmallCapConfig)\n",
    "    AutoModel.register(SmallCapConfig, SmallCap)\n",
    "    model = AutoModel.from_pretrained(\"Yova/SmallCap7M\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # count trainable parameters\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    num_trainable_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    print('Training a model with {} trainable parameters.'.format(num_trainable_params))\n",
    "\n",
    "    return model, tokenizer, feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(features_dir, annotations_path, experiments_dir, encoder_name, decoder_name, attention_size, train_decoder,\n",
    "     disable_rag, k, retrieval_encoder, captions_path, template_path, n_epochs, lr, batch_size, gradient_steps):\n",
    "\n",
    "    model, tokenizer, feature_extractor = get_model_and_auxiliaries()\n",
    "    train_dataset = get_data(tokenizer, model.config.max_length, features_dir, annotations_path, experiments_dir, \\\n",
    "                            encoder_name, decoder_name, attention_size, train_decoder, disable_rag, k, retrieval_encoder, \\\n",
    "                            captions_path, template_path, n_epochs, lr, batch_size, gradient_steps)\n",
    "\n",
    "    model_type = 'norag' \n",
    "\n",
    "    output_dir = '{}_{}M_{}'.format(model_type, attention_size, decoder_name)\n",
    "    output_dir = os.path.join(experiments_dir, output_dir)\n",
    "    \n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        num_train_epochs=n_epochs, \n",
    "        per_device_train_batch_size=batch_size, \n",
    "        gradient_accumulation_steps=gradient_steps,\n",
    "        learning_rate = lr,\n",
    "        fp16=True,\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=n_epochs, \n",
    "        logging_strategy=\"epoch\", \n",
    "        output_dir=output_dir, \n",
    "        overwrite_output_dir=True, \n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=default_data_collator, \n",
    "        train_dataset=train_dataset,\n",
    "        tokenizer=feature_extractor,\n",
    "    )\n",
    "\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading feature extractor configuration file https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/preprocessor_config.json from cache at /home/ec2-user/.cache/huggingface/transformers/70a7f0560266197c6c30797348b0d05575ccc9efe7b0025815d2f9800d223f7a.cc322732a6b876aad7dc614c3b22104fcc07458dc2df81953cff99baed365f9a\n",
      "Feature extractor CLIPFeatureExtractor {\n",
      "  \"crop_size\": 224,\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"CLIPFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"size\": 224\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/facebook/opt-350m/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/d607f52c6959c98fd5116288e3118e4b536b5336e14e43b9893f467247c51903.1df7a93756b24a5859673be20e064b5c30771c0c8ca9e6a56ff30daa2885580b\n",
      "Model config OPTConfig {\n",
      "  \"_name_or_path\": \"facebook/opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/facebook/opt-350m/resolve/main/vocab.json from cache at /home/ec2-user/.cache/huggingface/transformers/1c01596e83771026cb699d1cae3730757368e751a18e12e0e7f750698047ef40.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n",
      "loading file https://huggingface.co/facebook/opt-350m/resolve/main/merges.txt from cache at /home/ec2-user/.cache/huggingface/transformers/623fd69aa49a13c8fff1efb21297e15e6cf5dbc00cb5401a3ce777513dac9996.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/facebook/opt-350m/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/facebook/opt-350m/resolve/main/special_tokens_map.json from cache at /home/ec2-user/.cache/huggingface/transformers/b81dc485be36e6b937ed2f0625667420e8fc3600aeebceb71e239f486c368c41.c7cc7d24e97c79eaf304e87679fffb4f36cf739d549738da5cc604bf047de6ce\n",
      "loading file https://huggingface.co/facebook/opt-350m/resolve/main/tokenizer_config.json from cache at /home/ec2-user/.cache/huggingface/transformers/6ca50bf036999ee00d7350976bf0635ebd81402ffe912e2973219d7f3c02ae6b.54de7f5b4b5bd7e3ac5035740eb559a2d3ae70659ba65ce7610b9999cd20dc53\n",
      "loading configuration file https://huggingface.co/facebook/opt-350m/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/d607f52c6959c98fd5116288e3118e4b536b5336e14e43b9893f467247c51903.1df7a93756b24a5859673be20e064b5c30771c0c8ca9e6a56ff30daa2885580b\n",
      "Model config OPTConfig {\n",
      "  \"_name_or_path\": \"facebook/opt-350m\",\n",
      "  \"_remove_final_layer_norm\": false,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"OPTForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"do_layer_norm_before\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_dim\": 4096,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"init_std\": 0.02,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"opt\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \"</s>\",\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50272,\n",
      "  \"word_embed_proj_dim\": 512\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/Yova/SmallCap7M/resolve/main/config.json from cache at /home/ec2-user/.cache/huggingface/transformers/088a7d216ddc49a2d7284779886466dbd4a2bed648e2e6ec361e474883a6d210.0a3db4f7acc9d378534d91e44428850a95135745bfcabc7ecf4e12e67928c920\n",
      "text_config_dict is None. Initializing the CLIPTextConfig with default values.\n",
      "vision_config_dict is None. initializing the CLIPVisionConfig with default values.\n",
      "Model config SmallCapConfig {\n",
      "  \"_name_or_path\": \"Yova/SmallCap7M\",\n",
      "  \"architectures\": [\n",
      "    \"SmallCap\"\n",
      "  ],\n",
      "  \"decoder\": {\n",
      "    \"_name_or_path\": \"this_gpt2\",\n",
      "    \"activation_function\": \"gelu_new\",\n",
      "    \"add_cross_attention\": true,\n",
      "    \"architectures\": [\n",
      "      \"ThisGPT2LMHeadModel\"\n",
      "    ],\n",
      "    \"attn_pdrop\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"bos_token_id\": 50256,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"cross_attention_reduce_factor\": 4,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"embd_pdrop\": 0.1,\n",
      "    \"encoder_hidden_size\": 768,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 50256,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"is_decoder\": true,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_epsilon\": 1e-05,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"this_gpt2\",\n",
      "    \"n_ctx\": 1024,\n",
      "    \"n_embd\": 768,\n",
      "    \"n_head\": 12,\n",
      "    \"n_inner\": null,\n",
      "    \"n_layer\": 12,\n",
      "    \"n_positions\": 1024,\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": null,\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"reorder_and_upcast_attn\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"resid_pdrop\": 0.1,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"scale_attn_by_inverse_layer_idx\": false,\n",
      "    \"scale_attn_weights\": true,\n",
      "    \"sep_token_id\": null,\n",
      "    \"summary_activation\": null,\n",
      "    \"summary_first_dropout\": 0.1,\n",
      "    \"summary_proj_to_labels\": true,\n",
      "    \"summary_type\": \"cls_index\",\n",
      "    \"summary_use_proj\": true,\n",
      "    \"task_specific_params\": {\n",
      "      \"text-generation\": {\n",
      "        \"do_sample\": true,\n",
      "        \"max_length\": 50\n",
      "      }\n",
      "    },\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.21.1\",\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 50257\n",
      "  },\n",
      "  \"encoder\": {\n",
      "    \"_name_or_path\": \"openai/clip-vit-base-patch32\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": [\n",
      "      \"CLIPModel\"\n",
      "    ],\n",
      "    \"bad_words_ids\": null,\n",
      "    \"bos_token_id\": null,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": null,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_factor\": 1.0,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"logit_scale_init_value\": 2.6592,\n",
      "    \"max_length\": 20,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"clip\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": null,\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"projection_dim\": 512,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"text_config\": {\n",
      "      \"_name_or_path\": \"\",\n",
      "      \"add_cross_attention\": false,\n",
      "      \"architectures\": null,\n",
      "      \"attention_dropout\": 0.0,\n",
      "      \"bad_words_ids\": null,\n",
      "      \"bos_token_id\": 0,\n",
      "      \"chunk_size_feed_forward\": 0,\n",
      "      \"cross_attention_hidden_size\": null,\n",
      "      \"decoder_start_token_id\": null,\n",
      "      \"diversity_penalty\": 0.0,\n",
      "      \"do_sample\": false,\n",
      "      \"dropout\": 0.0,\n",
      "      \"early_stopping\": false,\n",
      "      \"encoder_no_repeat_ngram_size\": 0,\n",
      "      \"eos_token_id\": 2,\n",
      "      \"exponential_decay_length_penalty\": null,\n",
      "      \"finetuning_task\": null,\n",
      "      \"forced_bos_token_id\": null,\n",
      "      \"forced_eos_token_id\": null,\n",
      "      \"hidden_act\": \"quick_gelu\",\n",
      "      \"hidden_size\": 512,\n",
      "      \"id2label\": {\n",
      "        \"0\": \"LABEL_0\",\n",
      "        \"1\": \"LABEL_1\"\n",
      "      },\n",
      "      \"initializer_factor\": 1.0,\n",
      "      \"initializer_range\": 0.02,\n",
      "      \"intermediate_size\": 2048,\n",
      "      \"is_decoder\": false,\n",
      "      \"is_encoder_decoder\": false,\n",
      "      \"label2id\": {\n",
      "        \"LABEL_0\": 0,\n",
      "        \"LABEL_1\": 1\n",
      "      },\n",
      "      \"layer_norm_eps\": 1e-05,\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 20,\n",
      "      \"max_position_embeddings\": 77,\n",
      "      \"min_length\": 0,\n",
      "      \"model_type\": \"clip_text_model\",\n",
      "      \"no_repeat_ngram_size\": 0,\n",
      "      \"num_attention_heads\": 8,\n",
      "      \"num_beam_groups\": 1,\n",
      "      \"num_beams\": 1,\n",
      "      \"num_hidden_layers\": 12,\n",
      "      \"num_return_sequences\": 1,\n",
      "      \"output_attentions\": false,\n",
      "      \"output_hidden_states\": false,\n",
      "      \"output_scores\": false,\n",
      "      \"pad_token_id\": 1,\n",
      "      \"prefix\": null,\n",
      "      \"problem_type\": null,\n",
      "      \"pruned_heads\": {},\n",
      "      \"remove_invalid_values\": false,\n",
      "      \"repetition_penalty\": 1.0,\n",
      "      \"return_dict\": true,\n",
      "      \"return_dict_in_generate\": false,\n",
      "      \"sep_token_id\": null,\n",
      "      \"task_specific_params\": null,\n",
      "      \"temperature\": 1.0,\n",
      "      \"tf_legacy_loss\": false,\n",
      "      \"tie_encoder_decoder\": false,\n",
      "      \"tie_word_embeddings\": true,\n",
      "      \"tokenizer_class\": null,\n",
      "      \"top_k\": 50,\n",
      "      \"top_p\": 1.0,\n",
      "      \"torch_dtype\": null,\n",
      "      \"torchscript\": false,\n",
      "      \"transformers_version\": \"4.21.1\",\n",
      "      \"typical_p\": 1.0,\n",
      "      \"use_bfloat16\": false,\n",
      "      \"vocab_size\": 49408\n",
      "    },\n",
      "    \"text_config_dict\": null,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": null,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"vision_config\": {\n",
      "      \"_name_or_path\": \"\",\n",
      "      \"add_cross_attention\": false,\n",
      "      \"architectures\": null,\n",
      "      \"attention_dropout\": 0.0,\n",
      "      \"bad_words_ids\": null,\n",
      "      \"bos_token_id\": null,\n",
      "      \"chunk_size_feed_forward\": 0,\n",
      "      \"cross_attention_hidden_size\": null,\n",
      "      \"decoder_start_token_id\": null,\n",
      "      \"diversity_penalty\": 0.0,\n",
      "      \"do_sample\": false,\n",
      "      \"dropout\": 0.0,\n",
      "      \"early_stopping\": false,\n",
      "      \"encoder_no_repeat_ngram_size\": 0,\n",
      "      \"eos_token_id\": null,\n",
      "      \"exponential_decay_length_penalty\": null,\n",
      "      \"finetuning_task\": null,\n",
      "      \"forced_bos_token_id\": null,\n",
      "      \"forced_eos_token_id\": null,\n",
      "      \"hidden_act\": \"quick_gelu\",\n",
      "      \"hidden_size\": 768,\n",
      "      \"id2label\": {\n",
      "        \"0\": \"LABEL_0\",\n",
      "        \"1\": \"LABEL_1\"\n",
      "      },\n",
      "      \"image_size\": 224,\n",
      "      \"initializer_factor\": 1.0,\n",
      "      \"initializer_range\": 0.02,\n",
      "      \"intermediate_size\": 3072,\n",
      "      \"is_decoder\": false,\n",
      "      \"is_encoder_decoder\": false,\n",
      "      \"label2id\": {\n",
      "        \"LABEL_0\": 0,\n",
      "        \"LABEL_1\": 1\n",
      "      },\n",
      "      \"layer_norm_eps\": 1e-05,\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 20,\n",
      "      \"min_length\": 0,\n",
      "      \"model_type\": \"clip_vision_model\",\n",
      "      \"no_repeat_ngram_size\": 0,\n",
      "      \"num_attention_heads\": 12,\n",
      "      \"num_beam_groups\": 1,\n",
      "      \"num_beams\": 1,\n",
      "      \"num_channels\": 3,\n",
      "      \"num_hidden_layers\": 12,\n",
      "      \"num_return_sequences\": 1,\n",
      "      \"output_attentions\": false,\n",
      "      \"output_hidden_states\": false,\n",
      "      \"output_scores\": false,\n",
      "      \"pad_token_id\": null,\n",
      "      \"patch_size\": 32,\n",
      "      \"prefix\": null,\n",
      "      \"problem_type\": null,\n",
      "      \"pruned_heads\": {},\n",
      "      \"remove_invalid_values\": false,\n",
      "      \"repetition_penalty\": 1.0,\n",
      "      \"return_dict\": true,\n",
      "      \"return_dict_in_generate\": false,\n",
      "      \"sep_token_id\": null,\n",
      "      \"task_specific_params\": null,\n",
      "      \"temperature\": 1.0,\n",
      "      \"tf_legacy_loss\": false,\n",
      "      \"tie_encoder_decoder\": false,\n",
      "      \"tie_word_embeddings\": true,\n",
      "      \"tokenizer_class\": null,\n",
      "      \"top_k\": 50,\n",
      "      \"top_p\": 1.0,\n",
      "      \"torch_dtype\": null,\n",
      "      \"torchscript\": false,\n",
      "      \"transformers_version\": \"4.21.1\",\n",
      "      \"typical_p\": 1.0,\n",
      "      \"use_bfloat16\": false\n",
      "    },\n",
      "    \"vision_config_dict\": null\n",
      "  },\n",
      "  \"eos_token_id\": 13,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 143,\n",
      "  \"model_type\": \"smallcap\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": null,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/Yova/SmallCap7M/resolve/main/pytorch_model.bin from cache at /home/ec2-user/.cache/huggingface/transformers/375eeecadb97edcc3b9266714c318c199036eca6bf8001c5c75b0255ca6d466b.bbd844511c23da054d14206e65cd1ede10b8980a3030488b117bda3259ef876a\n",
      "All model checkpoint weights were used when initializing SmallCap.\n",
      "\n",
      "All the weights of SmallCap were initialized from the model checkpoint at Yova/SmallCap7M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use SmallCap for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a model with 257605632 trainable parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using cuda_amp half precision backend\n",
      "/opt/conda/envs/smallcap/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 11565\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 92520\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='92520' max='92520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [92520/92520 2:26:51, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11565</td>\n",
       "      <td>2.197700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23130</td>\n",
       "      <td>1.388600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34695</td>\n",
       "      <td>1.042400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46260</td>\n",
       "      <td>0.808900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57825</td>\n",
       "      <td>0.646400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69390</td>\n",
       "      <td>0.528900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80955</td>\n",
       "      <td>0.437100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92520</td>\n",
       "      <td>0.365500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to experiments/norag_7M_facebook/opt-350m/checkpoint-11565\n",
      "Configuration saved in experiments/norag_7M_facebook/opt-350m/checkpoint-11565/config.json\n",
      "Model weights saved in experiments/norag_7M_facebook/opt-350m/checkpoint-11565/pytorch_model.bin\n",
      "Feature extractor saved in experiments/norag_7M_facebook/opt-350m/checkpoint-11565/preprocessor_config.json\n",
      "Deleting older checkpoint [experiments/norag_7M_facebook/opt-350m/checkpoint-11565] due to args.save_total_limit\n",
      "Saving model checkpoint to experiments/norag_7M_facebook/opt-350m/checkpoint-23130\n",
      "Configuration saved in experiments/norag_7M_facebook/opt-350m/checkpoint-23130/config.json\n",
      "Model weights saved in experiments/norag_7M_facebook/opt-350m/checkpoint-23130/pytorch_model.bin\n",
      "Feature extractor saved in experiments/norag_7M_facebook/opt-350m/checkpoint-23130/preprocessor_config.json\n",
      "Saving model checkpoint to experiments/norag_7M_facebook/opt-350m/checkpoint-34695\n",
      "Configuration saved in experiments/norag_7M_facebook/opt-350m/checkpoint-34695/config.json\n",
      "Model weights saved in experiments/norag_7M_facebook/opt-350m/checkpoint-34695/pytorch_model.bin\n",
      "Feature extractor saved in experiments/norag_7M_facebook/opt-350m/checkpoint-34695/preprocessor_config.json\n",
      "Saving model checkpoint to experiments/norag_7M_facebook/opt-350m/checkpoint-46260\n",
      "Configuration saved in experiments/norag_7M_facebook/opt-350m/checkpoint-46260/config.json\n",
      "Model weights saved in experiments/norag_7M_facebook/opt-350m/checkpoint-46260/pytorch_model.bin\n",
      "Feature extractor saved in experiments/norag_7M_facebook/opt-350m/checkpoint-46260/preprocessor_config.json\n",
      "Saving model checkpoint to experiments/norag_7M_facebook/opt-350m/checkpoint-57825\n",
      "Configuration saved in experiments/norag_7M_facebook/opt-350m/checkpoint-57825/config.json\n",
      "Model weights saved in experiments/norag_7M_facebook/opt-350m/checkpoint-57825/pytorch_model.bin\n",
      "Feature extractor saved in experiments/norag_7M_facebook/opt-350m/checkpoint-57825/preprocessor_config.json\n",
      "Saving model checkpoint to experiments/norag_7M_facebook/opt-350m/checkpoint-69390\n",
      "Configuration saved in experiments/norag_7M_facebook/opt-350m/checkpoint-69390/config.json\n",
      "Model weights saved in experiments/norag_7M_facebook/opt-350m/checkpoint-69390/pytorch_model.bin\n",
      "Feature extractor saved in experiments/norag_7M_facebook/opt-350m/checkpoint-69390/preprocessor_config.json\n",
      "Saving model checkpoint to experiments/norag_7M_facebook/opt-350m/checkpoint-80955\n",
      "Configuration saved in experiments/norag_7M_facebook/opt-350m/checkpoint-80955/config.json\n",
      "Model weights saved in experiments/norag_7M_facebook/opt-350m/checkpoint-80955/pytorch_model.bin\n",
      "Feature extractor saved in experiments/norag_7M_facebook/opt-350m/checkpoint-80955/preprocessor_config.json\n",
      "Saving model checkpoint to experiments/norag_7M_facebook/opt-350m/checkpoint-92520\n",
      "Configuration saved in experiments/norag_7M_facebook/opt-350m/checkpoint-92520/config.json\n",
      "Model weights saved in experiments/norag_7M_facebook/opt-350m/checkpoint-92520/pytorch_model.bin\n",
      "Feature extractor saved in experiments/norag_7M_facebook/opt-350m/checkpoint-92520/preprocessor_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Waiting for W&B process to finish... (success).\n"
     ]
    }
   ],
   "source": [
    "# Define arguments\n",
    "\n",
    "#Directory where cached input image features are stored\n",
    "features_dir =  'fine-tuning/features/'\n",
    "\n",
    "#JSON file with annotations\n",
    "annotations_path =  'fine-tuning/datastore/index.json'  \n",
    "\n",
    "#Directory where trained models will be saved\n",
    "experiments_dir =  \"experiments/\"  \n",
    "\n",
    "encoder_name = \"openai/clip-vit-base-patch32\"  #Encoder name as found of HuggingFace or stored locally\n",
    "decoder_name = \"facebook/opt-350m\"             #Decoder name as found of HuggingFace or stored locally\n",
    "attention_size = 7                             #Number of parameters in the cross attention {28, 14, 7, 3.5, 1.75}\n",
    "train_decoder = False                          #Whether to train the decoder in addition to the attention\n",
    "\n",
    "disable_rag = False                            #Disable retrieval augmentation\n",
    "k = 4                                          #Number of retrieved captions to use in prefix\n",
    "retrieval_encoder = \"RN50x64\"                  #Visual encoder used for retieving captions\n",
    "\n",
    "#JSON file with retrieved captions\n",
    "captions_path = 'fine-tuning/data/retrieved_caps_resnet50x64.json'      \n",
    "\n",
    "#TXT file with template\n",
    "template_path = \"src/template.txt\"             \n",
    "\n",
    "n_epochs = 8                                   #Number of training epochs\n",
    "lr = 1e-4                                      #Learning rate\n",
    "batch_size = 1                                 #Batch size\n",
    "gradient_steps = 1                             #Number of gradient accumulation steps\n",
    "\n",
    "# Call the main function with defined arguments\n",
    "main(features_dir, annotations_path, experiments_dir, encoder_name, decoder_name, attention_size, train_decoder,\n",
    "     disable_rag, k, retrieval_encoder, captions_path, template_path, n_epochs, lr, batch_size, gradient_steps)\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
